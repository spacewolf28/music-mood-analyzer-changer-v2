# backend/dsp/style_accompaniment/pipeline/rock_remix_builder.py
from pathlib import Path
import numpy as np
import librosa

# ä½ è‡ªå·±çš„ä¸¤ä¸ªæ¨¡å‹æ¥å£ï¼ˆä¿æŒå’Œé¡¹ç›®é‡Œä¸€è‡´ï¼‰
from backend.inference.style_recognition import predict_style
from backend.inference.emotion_recognition import predict_emotion

# æˆ‘ä»¬åˆšåˆšå†™çš„è¿™äº›æ¨¡å—
from backend.dsp.style_accompaniment.brain.rock_params_ai import RockParamsAI
from backend.dsp.style_accompaniment.generators.rock_drum_generator import RockDrumGenerator
from backend.dsp.style_accompaniment.generators.rock_bass_generator import RockBassGenerator
from backend.dsp.style_accompaniment.generators.rock_guitar_generator import RockGuitarGenerator
from backend.dsp.style_accompaniment.mixer.smart_mixer import SmartMixer


class RockRemixBuilder:
    """
    å®Œæ•´ Rock é£æ ¼è½¬æ¢æ€»æ§ï¼š

        è¾“å…¥ç”¨æˆ·çš„åŸå§‹éŸ³ä¹ audio_path
      -> ä½¿ç”¨ä½ è®­ç»ƒçš„ style / emotion æ¨¡å‹åˆ†æåŸæ›²
      -> æå–åŸæ›²çš„ tempo / energy / brightness
      -> RockParamsAI æ ¹æ®ã€ç›®æ ‡é£æ ¼ + ç›®æ ‡æƒ…ç»ª + åŸæ›²ç‰¹å¾ã€‘ç”Ÿæˆå‚æ•°
      -> å‰ä»– / Bass / é¼“ ç”Ÿæˆå¯¹åº”è½¨é“
      -> SmartMixer æ··æˆä¸€æ¡ Rock ä¼´å¥

    æ³¨æ„ï¼šç›®å‰ç›®æ ‡é£æ ¼å›ºå®šä¸º "rock"ï¼Œåé¢å¯ä»¥æ‰©å±•æˆå¤šé£æ ¼ã€‚
    """

    def __init__(self,
                 length_s: float = 10.0,
                 target_style: str = "rock"):
        """
        :param length_s: ç”ŸæˆéŸ³ä¹é•¿åº¦ï¼ˆç§’ï¼‰
        :param target_style: ç›®æ ‡é£æ ¼ï¼Œç›®å‰å…ˆå›ºå®š "rock"
        """
        self.length_s = float(length_s)
        self.target_style = target_style

        # æ ¸å¿ƒæ¨ç† AI
        self.params_ai = RockParamsAI()

        # ç”Ÿæˆå™¨
        self.drum_gen = RockDrumGenerator()
        self.bass_gen = RockBassGenerator()
        self.guitar_gen = RockGuitarGenerator()

        # æ™ºèƒ½æ··éŸ³
        self.mixer = SmartMixer()

    # -------------------- å†…éƒ¨ï¼šæ¨¡å‹å°è£… --------------------

    def _run_style_model(self, audio_path: str):
        """
        å°è£…ä¸€ä¸‹ style æ¨¡å‹ï¼Œå…¼å®¹ å­—ç¬¦ä¸² / (label, prob) / dict è¾“å‡ºã€‚
        """
        res = predict_style(audio_path)
        if isinstance(res, dict):
            label = res.get("label") or res.get("style")
            probs = res.get("probs") or res.get("prob")
            return str(label), probs
        if isinstance(res, (list, tuple)):
            if len(res) >= 2:
                return str(res[0]), res[1]
            return str(res[0]), None
        return str(res), None

    def _run_emotion_model(self, audio_path: str):
        """
        å°è£…ä¸€ä¸‹ emotion æ¨¡å‹ï¼Œå…¼å®¹ å­—ç¬¦ä¸² / (label, prob) / dict è¾“å‡ºã€‚
        """
        res = predict_emotion(audio_path)
        if isinstance(res, dict):
            label = res.get("label") or res.get("emotion")
            probs = res.get("probs") or res.get("prob")
            return str(label), probs
        if isinstance(res, (list, tuple)):
            if len(res) >= 2:
                return str(res[0]), res[1]
            return str(res[0]), None
        return str(res), None

    def _extract_basic_features(self, audio_path: str) -> dict:
        """
        ä»åŸæ›²ä¸­æå–ï¼š
            - tempoï¼ˆå¤§æ¦‚çš„ BPMï¼‰
            - energyï¼ˆèƒ½é‡ï¼ŒåŸºäº RMSï¼‰
            - brightnessï¼ˆäº®åº¦ï¼ŒåŸºäºè°±å¿ƒï¼‰
        è¿™äº›æ˜¯æ–¹å‘ A ä¸­ test_audio çš„â€œé«˜å±‚ç‰¹å¾ä½œç”¨â€ã€‚
        """
        y, sr = librosa.load(audio_path, sr=None, mono=True)

        # tempo
        tempo, _ = librosa.beat.beat_track(y=y, sr=sr)
        if tempo <= 0:
            tempo = 120.0

        # energy
        rms = librosa.feature.rms(y=y).mean()
        energy = float(np.clip(rms / 0.1, 0.0, 1.0))

        # brightness
        centroid = librosa.feature.spectral_centroid(y=y, sr=sr).mean()
        brightness = float(np.clip(centroid / 6000.0, 0.0, 1.0))

        return {
            "tempo": float(tempo),
            "energy": energy,
            "brightness": brightness,
        }

    # -------------------- å¯¹å¤–ä¸»å…¥å£ --------------------

    def build(self,
              audio_path: str,
              target_emotion: str | None = None) -> str:
        """
        :param audio_path: ç”¨æˆ·ä¸Šä¼ çš„åŸå§‹éŸ³ä¹è·¯å¾„
        :param target_emotion: ç›®æ ‡æƒ…ç»ªï¼Œå¦‚æœä¸º None å°±ç”¨æ¨¡å‹è¯†åˆ«åˆ°çš„ emotion
        :return: æœ€ç»ˆç”Ÿæˆçš„ rock ä¼´å¥ wav è·¯å¾„
        """
        audio_path = str(Path(audio_path).resolve())
        print(f"[RockBuilder] è¾“å…¥éŸ³é¢‘: {audio_path}")

        # 1) ä½¿ç”¨ä½ è®­ç»ƒçš„æ¨¡å‹åˆ†æåŸæ›²
        original_style, style_probs = self._run_style_model(audio_path)
        original_emotion, emo_probs = self._run_emotion_model(audio_path)

        print(f"[RockBuilder] åŸæ›²è¯†åˆ«ç»“æœ: style={original_style}, emotion={original_emotion}")

        # å¦‚æœç”¨æˆ·æŒ‡å®šäº†ç›®æ ‡æƒ…ç»ªï¼Œå°±è¦†ç›–
        used_emotion = target_emotion or original_emotion
        used_style = self.target_style or original_style

        print(f"[RockBuilder] ç›®æ ‡é£æ ¼: {used_style}, ç›®æ ‡æƒ…ç»ª: {used_emotion}")

        # 2) æå–åŸæ›²èƒ½é‡ / äº®åº¦ / tempo
        features = self._extract_basic_features(audio_path)
        print(f"[RockBuilder] æå–ç‰¹å¾: {features}")

        # 3) ç”± RockParamsAI ç”Ÿæˆæ‰€æœ‰å‚æ•°ï¼ˆçœŸæ­£çš„ AI æ¨ç†æ ¸å¿ƒï¼‰
        params = self.params_ai.generate_all_params(
            style=used_style,
            emotion=used_emotion,
            features=features
        )

        print(f"[RockBuilder] AI ç”Ÿæˆå‚æ•°ç»„åˆ: {params['combo']}")
        print(f"[RockBuilder] Guitar Params: {params['guitar']}")
        print(f"[RockBuilder] Bass Params:   {params['bass']}")
        print(f"[RockBuilder] Drums Params:  {params['drums']}")
        print(f"[RockBuilder] Mix Params:    {params['mix']}")

        tempo = params["tempo"]

        # 4) ç”Ÿæˆæ¯ä¸€æ¡ä¹å™¨è½¨é“
        print("[RockBuilder] ç”Ÿæˆé¼“è½¨...")
        drum_path = self.drum_gen.generate(
            tempo=tempo,
            drum_params=params["drums"],
            length_s=self.length_s
        )

        print("[RockBuilder] ç”Ÿæˆ Bass è½¨...")
        bass_path = self.bass_gen.generate(
            tempo=tempo,
            bass_params=params["bass"],
            length_s=self.length_s
        )

        print("[RockBuilder] ç”Ÿæˆå‰ä»–è½¨...")
        guitar_path = self.guitar_gen.generate(
            guitar_params=params["guitar"],
            length_s=self.length_s
        )

        # 5) æ™ºèƒ½æ··éŸ³
        print("[RockBuilder] æ™ºèƒ½æ··éŸ³ä¸­...")
        final_path = self.mixer.mix(
            stems={
                "drums": drum_path,
                "bass":  bass_path,
                "guitar": guitar_path,
            },
            mix_params=params["mix"]
        )

        print(f"[RockBuilder] âœ… å®Œæˆ Rock ä¼´å¥ç”Ÿæˆ: {final_path}")
        return final_path


if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(
        description="Rock Style Conversion (Direction A) - Command Line Runner"
    )

    parser.add_argument(
        "-i", "--input",
        type=str,
        required=True,
        help="è¾“å…¥éŸ³é¢‘æ–‡ä»¶è·¯å¾„"
    )

    parser.add_argument(
        "-e", "--emotion",
        type=str,
        default=None,
        help="ç›®æ ‡æƒ…ç»ªï¼ˆhappy/sad/angry/tender/funny/scaryï¼‰ã€‚ä¸å¡«åˆ™ä½¿ç”¨æ¨¡å‹è¯†åˆ«"
    )

    parser.add_argument(
        "-l", "--length",
        type=float,
        default=10.0,
        help="ç”Ÿæˆé•¿åº¦ï¼ˆç§’ï¼‰"
    )

    args = parser.parse_args()

    builder = RockRemixBuilder(length_s=args.length, target_style="rock")

    print(f"ğŸµ ç”Ÿæˆé£æ ¼: rock\nğŸ­ ç›®æ ‡æƒ…ç»ª: {args.emotion}\nğŸ“„ è¾“å…¥: {args.input}")

    out = builder.build(
        audio_path=args.input,
        target_emotion=args.emotion
    )

    print(f"ğŸ‰ è¾“å‡ºæ–‡ä»¶: {out}")

