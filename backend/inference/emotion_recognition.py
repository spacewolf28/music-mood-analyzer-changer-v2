import os
import numpy as np
import joblib
import librosa

from backend.features.yamnet_extract import extract_yamnet_embedding

# === è·¯å¾„ ===
MODEL_PATH = "backend/models/emotion_model.pkl"

# === åŠ è½½æ¨¡å‹ ===
emotion_model = joblib.load(MODEL_PATH)

# === ä½ è‡ªå·±çš„æ ‡ç­¾é¡ºåº ===
emotion_labels = [
    "angry",
    "funny",
    "happy",
    "sad",
    "scary",
    "tender"
]



def predict_emotion(audio_path: str):
    """è¾“å…¥éŸ³é¢‘è·¯å¾„ï¼Œè¿”å›æƒ…ç»ªåç§°"""

    if not os.path.exists(audio_path):
        raise FileNotFoundError(f"Audio file not found: {audio_path}")

    # 1. æå– YAMNet embedding
    embedding = extract_yamnet_embedding(audio_path)

    # 2. æœ‰äº› embedding æ˜¯å¤šå¸§ï¼Œå–å¹³å‡ï¼ˆè®­ç»ƒæ—¶ä¹Ÿæ˜¯è¿™ä¹ˆåšçš„ï¼‰
    if len(embedding.shape) > 1:
        embedding = embedding.mean(axis=0)

    embedding = embedding.reshape(1, -1)

    # 3. æ¨¡å‹é¢„æµ‹
    pred_idx = emotion_model.predict(embedding)[0]
    emotion = emotion_labels[pred_idx]

    return emotion


if __name__ == "__main__":
    test_audio = "backend/test_audio.wav"

    print("ğŸ” æ­£åœ¨åˆ†ææƒ…ç»ª...")
    emotion = predict_emotion(test_audio)
    print(f"ğŸµ è¯†åˆ«ç»“æœï¼š{emotion}")
